### slurm.conf controls the config for Slurm both in terms of the slurmctld
### daemon but also for slurmd on the clients. This also controls node and
### partition definition.

############################## Global #####################################
### This section sets global definitions

### Name of the cluster.
ClusterName=sid

### User that slurm will run as.
SlurmUser=slurm

### Width of the communications heirarchy.  Recommended to be either N^(1/2)
### or N^(1/3) where N is the number of nodes.
### Currently we set TreeWidth to N^(1/3) due to the fact we are spanning
### multiple datacenters that have significant latency penalty.
TreeWidth=12
MessageTimeout=100
RoutePlugin=route/topology
TopologyPlugin=topology/tree

### We don't set a default version of MPI.
MpiDefault=none

SwitchType=switch/none

### scron settings
ScronParameters=enable

### Logging
#SlurmctldLogFile=/var/log/messages
SlurmctldSyslogDebug=verbose
#DebugFlags=Route

### Prolog that the slurmctld runs when it schedules jobs.
#PrologSlurmctld=/usr/local/sbin/slurmctld_prolog

### Where are are logging data about job completions.
JobCompHost=holy-slurm02
JobCompType=jobcomp/filetxt

########################## slurmd #########################################
### This section sets the definitions specific to the slurmd and cgroups.

SlurmdPort=6818
SrunPortRange=7845-11845
SlurmdPidFile=/var/run/slurmd.pid
SlurmdSpoolDir=/var/slurmd/spool/slurmd
SlurmdTimeout=300

### Logging
SlurmdSyslogDebug=verbose
SlurmdParameters=config_overrides

### We are using cgroups to track things and mange tasks.
ProctrackType=proctrack/cgroup
TaskPlugin=task/affinity,task/cgroup

### We have MCS turned on so that we can run isolated jobs.
MCSPlugin=mcs/account
MCSParameters=ondemand,ondemandselect,privatedata

### What gres types we are using.
GresTypes=gpu

### Where scratch space is located on the node.
TmpFs=/scratch

### Nodes will stay down until reopened by the admin.
### This prevents flapping by nodes.
ReturnToService=0

### This sets how the cgroup will be set up by the prolog script.
### We need X11 in order to use the Slurm PAM libraries and X11.
PrologFlags=Contain,X11
UsePAM=1

### Location of Epilog Script
Epilog=/usr/local/bin/slurm_epilog

### Node Health Check
HealthCheckInterval=500
HealthCheckNodeState=CYCLE
HealthCheckProgram=/usr/local/bin/node_monitor

############################ slurmdbd #####################################
### This section sets the definitions for slurmdbd and accounting.
AccountingStorageHost=holy-slurm02
AccountingStoragePort=6819
AccountingStorageUser=slurm
AccountingStorageType=accounting_storage/slurmdbd

### Determines what we track for accounting.
### AccountingStorageEnforce being set to safe means that if we enforce a
### GrpCPUMins limit, jobs will only launch if they will complete before
### that limit is exhuasted.  Else the jobs would simply terminate when the
### limit is hit regardless if the job still had time left.
AccountingStorageEnforce=safe
# DISABLING IB + LUSTRE ACCOUNTING FOR NOW UNTIL ISSUES RESOLVED
#AccountingStorageTRES=Billing,CPU,Energy,Mem,Node,FS/Disk,FS/Lustre,Pages,VMem,IC/OFED,gres/gpu
AccountingStorageTRES=Billing,CPU,Energy,Mem,Node,FS/Disk,Pages,VMem,gres/gpu
AccountingStoreJobComment=YES
# DISABLING IB + LUSTRE ACCOUNTING FOR NOW UNTIL ISSUES RESOLVED
#AcctGatherInfinibandType=acct_gather_infiniband/ofed
#AcctGatherFilesystemType=acct_gather_filesystem/lustre

### Determines how frequently we will ping for job data.
### We use jobacct_gather/linux rather than cgroups as that is what is
### recommended by slurm in their documentation even though we have
### cgroups enabled.
JobAcctGatherType=jobacct_gather/linux
JobAcctGatherFrequency=task=30,network=30,filesystem=30


########################## Scheduling #####################################
### This section is specific to scheduling

### Tells the scheduler to enforce limits for all partitions
### that a job submits to.
EnforcePartLimits=ALL

### Let's slurm know that we have a jobsubmit.lua script
JobSubmitPlugins=lua

### When a job is launched this locks the memory a user can use,
### it also sorts NUMA memory. In addition for srun invocations
### we test that the executable is actually available and has
### the correct permissions before launching.
### Also we permit salloc to launch interactive jobs
LaunchParameters=mem_sort,slurmstepd_memlock_all,test_exec,use_interactive_step

### Set's licenses we are running.
### These to not tie into FlexLM or our License server.
Licenses=lumerical:10,MATLAB_Distrib_Comp_Engine:256,renderman:29

### Maximum sizes for Jobs.
MaxJobCount=300000
MaxArraySize=10000
DefMemPerCPU=100
DefCpuPerGPU=1
DefMemPerGPU=100
GpuFreqDef=low

### Job Timers
CompleteWait=0

### We set the EpilogMsgTime long so that Epilog Messages don't pile up all
### at one time due to forced exit which can cause problems for the master.
EpilogMsgTime=3000000
InactiveLimit=0
KillWait=30

### This only applies to the reservation time limit, the job must still obey
### the partition time limit.
ResvOverRun=UNLIMITED
MinJobAge=600
Waittime=0

### Scheduling parameters
SchedulerType=sched/backfill
SelectType=select/cons_tres
SelectTypeParameters=CR_Core_Memory

### Govern's default preemption behavior
PreemptType=preempt/partition_prio
PreemptMode=REQUEUE

### default_queue_depth should be some multiple of the partition_job_depth,
### ideally number_of_partitions * partition_job_depth, but typically the main
### loop exits prematurely if you go over about 400. A partition_job_depth of
### 10 seems to work well.
SchedulerParameters=\
default_queue_depth=1500,\
partition_job_depth=10,\
bf_continue,\
bf_interval=30,\
bf_resolution=600,\
bf_window=11520,\
bf_max_job_part=0,\
bf_max_job_user=10,\
bf_max_job_test=100000,\
bf_max_job_start=1000,\
bf_ignore_newly_avail_nodes,\
enable_user_top,\
pack_serial_at_end,\
nohold_on_prolog_fail,\
permit_job_expansion,\
preempt_strict_order,\
preempt_youngest_first,\
reduce_completing_frag,\
max_rpc_cnt=16

### This controls how we handle dependencies. If we have invalid dependencies
### we terminate the job.
DependencyParameters=kill_invalid_depend

################################ Fairshare ################################
### This section sets the fairshare calculations

PriorityType=priority/multifactor
PriorityFlags=NO_FAIR_TREE

### Settings for fairshare calculation frequency and shape.
FairShareDampeningFactor=1
PriorityDecayHalfLife=28-0
PriorityCalcPeriod=1

### Settings for fairshare weighting.
PriorityMaxAge=7-0
PriorityWeightAge=10000000
PriorityWeightFairshare=20000000
PriorityWeightJobSize=0
PriorityWeightPartition=0
PriorityWeightQOS=1000000000

NodeName=aagk80gpu[01-02,08,12-38,40-45] \
    CPUs=12 RealMemory=128668 MemSpecLimit=4096 Sockets=2 CoresPerSocket=6 \
    ThreadsPerCore=1 TmpDisk=24160 \
    Feature=intel,holyib,haswell,avx,avx2,k80,cc3.5,cc3.7 Gres=gpu:4

PartitionName=sid-default State=UP PriorityTier=1 DefaultTime=0-00:10:00 \
    TRESBillingWeights="CPU=0.5,Mem=0.125G,Gres/gpu=37.5" \
	PreemptMode=REQUEUE MaxTime=7-0 Default=YES MaxNodes=1 \
	AllowGroups=cluster_users,cluster_users_2,slurm-admin \
	Nodes=\
aagk80gpu[08,12-38,40-45],\
bloxham-r940,\
holy2a103[01-16],\
shakgpu[01-02,04-15,17-20,22-50]

PartitionName=other-partition State=UP PreemptMode=OFF PriorityTier=4 DefaultTime=0-00:10:00 \
    TRESBillingWeights="CPU=1.0,Mem=0.25G" \
	AllowGroups=slurm-admin,other-lab \
	Nodes=holy7c184[01-02]

PartitionName=sid-partition1 State=UP PreemptMode=OFF PriorityTier=4 DefaultTime=0-00:10:00 \
    TRESBillingWeights="CPU=1.0,Mem=0.25G" \
	AllowGroups=sid-group,other-group \
	Nodes=holy7c184[01-02]

PartitionName=sid-partition2 State=UP PreemptMode=OFF PriorityTier=4 DefaultTime=0-00:10:00 \
    TRESBillingWeights="CPU=1.0,Mem=0.25G" \
	AllowGroups=sid-group,example-group \
	Nodes=holy7c184[01-02]
